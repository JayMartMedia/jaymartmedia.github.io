"use strict";(self.webpackChunkjaymartmedia=self.webpackChunkjaymartmedia||[]).push([[714],{7475:(a,e,l)=>{l.r(e),l.d(e,{assets:()=>m,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>s});var t=l(5893),n=l(1151);const o={slug:"running-llama-ai-model-locally",title:"Running Llama AI Model Locally",authors:"jaymartin",tags:["AI","Llama","Self hosting","Local"],toc_max_heading_level:4},i=void 0,r={permalink:"/blog/running-llama-ai-model-locally",editUrl:"https://github.com/jaymartmedia/jaymartmedia.github.io/tree/main/blog/2024-11-14-running-llama-ai-model-locally.md",source:"@site/blog/2024-11-14-running-llama-ai-model-locally.md",title:"Running Llama AI Model Locally",description:"The Llama AI model is an open-source model built by Meta. It can be used similarly to how OpenAI's ChatGPT can be used. In this article we will run the Llama AI model locally via Docker. We will NOT configure Llama to use the GPU in this article.",date:"2024-11-14T00:00:00.000Z",formattedDate:"November 14, 2024",tags:[{label:"AI",permalink:"/blog/tags/ai"},{label:"Llama",permalink:"/blog/tags/llama"},{label:"Self hosting",permalink:"/blog/tags/self-hosting"},{label:"Local",permalink:"/blog/tags/local"}],readingTime:4.18,hasTruncateMarker:!0,authors:[{name:"Jay Martin",title:"Runner of JayMartMedia",url:"https://github.com/jaymartmedia",imageURL:"https://github.com/jaymartmedia.png",key:"jaymartin"}],frontMatter:{slug:"running-llama-ai-model-locally",title:"Running Llama AI Model Locally",authors:"jaymartin",tags:["AI","Llama","Self hosting","Local"],toc_max_heading_level:4},unlisted:!1,prevItem:{title:"Call ChatGPT From NodeJS",permalink:"/blog/call-chatgpt-from-node-js"},nextItem:{title:"Setting up an NGINX reverse proxy",permalink:"/blog/nginx-reverse-proxy"}},m={authorsImageUrls:[void 0]},s=[];function c(a){const e={a:"a",p:"p",...(0,n.a)(),...a.components};return(0,t.jsxs)(e.p,{children:["The ",(0,t.jsx)(e.a,{href:"https://www.llama.com/",children:"Llama AI model"})," is an open-source model built by Meta. It can be used similarly to how OpenAI's ChatGPT can be used. In this article we will run the Llama AI model locally via Docker. We will NOT configure Llama to use the GPU in this article."]})}function u(a={}){const{wrapper:e}={...(0,n.a)(),...a.components};return e?(0,t.jsx)(e,{...a,children:(0,t.jsx)(c,{...a})}):c(a)}},1151:(a,e,l)=>{l.d(e,{Z:()=>r,a:()=>i});var t=l(7294);const n={},o=t.createContext(n);function i(a){const e=t.useContext(o);return t.useMemo((function(){return"function"==typeof a?a(e):{...e,...a}}),[e,a])}function r(a){let e;return e=a.disableParentContext?"function"==typeof a.components?a.components(n):a.components||n:i(a.components),t.createElement(o.Provider,{value:e},a.children)}}}]);